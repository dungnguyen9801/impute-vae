{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SHAPE=(32,32)\n",
    "BATCH_SIZE = 32\n",
    "DIM_Z = 50\n",
    "DIM_Z_HIDDEN = 25\n",
    "DIM_X_HIDDEN = 50\n",
    "DIM_X=np.prod(INPUT_SHAPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "tf.enable_eager_execution()\n",
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(dim_z_hidden, dim_z, input_shape, dim_x_hidden):\n",
    "    flatten_encode = keras.layers.Flatten()\n",
    "    dense_encode = keras.layers.Dense(dim_z_hidden, activation='tanh')\n",
    "    mu_encode = keras.layers.Dense(dim_z, activation='linear')\n",
    "    log_sigma_encode = keras.layers.Dense(dim_z, activation='linear')\n",
    "    inputs_encode = keras.layers.Input(shape=(*input_shape,))\n",
    "    encoder=keras.models.Model(\n",
    "        inputs=inputs_encode,\n",
    "        outputs=(\n",
    "            mu_encode(dense_encode(flatten_encode(inputs_encode))),\n",
    "            log_sigma_encode(dense_encode(flatten_encode(inputs_encode)))\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    dim_x = np.prod(input_shape)    \n",
    "    dense_decode = keras.layers.Dense(dim_x_hidden, activation='tanh')\n",
    "    mu_decode = keras.layers.Dense(dim_x, activation='linear')\n",
    "    log_sigma_decode = keras.layers.Dense(dim_x, activation='linear')\n",
    "    inputs_decode = keras.layers.Input(shape=(dim_z,))\n",
    "    decoder=keras.models.Model(\n",
    "        inputs=inputs_decode,\n",
    "        outputs=(\n",
    "            mu_decode(dense_decode((inputs_decode))),\n",
    "            log_sigma_decode(dense_decode((inputs_decode)))\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    return encoder, decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elbo(x,encoder, decoder, L=100, seed=0):\n",
    "    batch = x.shape[0]\n",
    "    mu_z, log_sigma_z = encoder(x)\n",
    "    \n",
    "    dim = mu_z.shape[1]\n",
    "    np.random.seed(seed)\n",
    "    eps = np.random.normal(0, 1, size = (L, batch, dim))\n",
    "    \n",
    "    zs = tf.reshape(eps *tf.exp(log_sigma_z) + mu_z, (-1, dim))\n",
    "    mu_x, log_sigma_x = decoder(zs) # (L * batch, dim_x)\n",
    "    mu_x = tf.reshape(mu_x, (L, batch, -1))\n",
    "    log_sigma_x = tf.reshape(log_sigma_x, (L, batch, -1))\n",
    "    \n",
    "    \n",
    "    minus_log_q = eps**2/2 + log_sigma_z + 0.5*tf.log(2*np.pi)\n",
    "    log_p = -(tf.dtypes.cast(tf.reshape(x, (batch, -1)), tf.float32)-mu_x)**2/(2 * tf.exp(2*log_sigma_x)) -log_sigma_x - 0.5*tf.log(2*np.pi)\n",
    "    log_pz = -zs**2/2 - 0.5*tf.log(2*np.pi)\n",
    "    return (tf.math.reduce_sum(log_p) + tf.math.reduce_sum(minus_log_q) + tf.math.reduce_sum(log_pz))/L\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bad_elbo(x,encoder, decoder, L=100, seed=0):\n",
    "    batch = x.shape[0]\n",
    "    mu_z, log_sigma_z = encoder(x)\n",
    "    sigma_z = np.exp(log_sigma_z)\n",
    "    dim = mu_z.shape[1]\n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    eps = np.random.normal(0,1, size = (L, batch, dim))\n",
    "    zs = eps * sigma_z + mu_z\n",
    "    mu_x, log_sigma_x = decoder(np.reshape(zs, (-1, dim)))\n",
    "    sigma_x = np.exp(log_sigma_x)\n",
    "    mu_x = np.reshape(mu_x, (L, batch, -1))\n",
    "    sigma_x = np.reshape(sigma_x, (L, batch, -1))\n",
    "    p_x_z = np.prod(np.exp(-zs**2/2)/np.sqrt(2*np.pi)) * np.prod(np.exp(-(np.reshape(x, (batch, -1))-mu_x)**2/(sigma_x**2*2))/np.sqrt(2*np.pi*sigma_x**2))\n",
    "    q_z_x = np.exp(-(zs-mu_z)**2/(sigma_z**2*2))/np.sqrt(2*np.pi*sigma_z**2)\n",
    "    return (np.log(np.prod(p_x_z)) - np.log(np.prod(q_z_x)))/L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def manual_elbo(x_, encoder, decoder, L=100, seed=0):\n",
    "    x = np.reshape(x_,(x_.shape[0], -1))\n",
    "    batch = x.shape[0]\n",
    "    encode_weights = encoder.get_weights()\n",
    "    encode_hidden = np.tanh(np.matmul(x, encode_weights[0]) + encode_weights[1])\n",
    "    mu_z = np.matmul(encode_hidden, encode_weights[2]) + encode_weights[3]\n",
    "    log_sigma_z = np.matmul(encode_hidden, encode_weights[4]) + encode_weights[5]\n",
    "    \n",
    "    decode_weights = decoder.get_weights()\n",
    "    dim = mu_z.shape[1]\n",
    "    np.random.seed(seed)\n",
    "    eps = np.random.normal(0, 1, size = (L, batch, dim))\n",
    "    zs = np.reshape(eps*np.exp(log_sigma_z) + mu_z, (-1, dim))\n",
    "    \n",
    "    decode_hidden = np.tanh(np.matmul(zs, decode_weights[0]) + decode_weights[1])\n",
    "    mu_x = np.matmul(decode_hidden, decode_weights[2]) + decode_weights[3]\n",
    "    log_sigma_x = np.matmul(decode_hidden, decode_weights[4]) + decode_weights[5]\n",
    "    mu_x = tf.reshape(mu_x, (L, batch, -1))\n",
    "    log_sigma_x = tf.reshape(log_sigma_x, (L, batch, -1))\n",
    "    \n",
    "    minus_log_q = eps**2/2 + log_sigma_z + 0.5*tf.log(2*np.pi)\n",
    "    log_p = -(x-mu_x)**2/(2 * np.exp(2*log_sigma_x)) -log_sigma_x - 0.5*np.log(2*np.pi)\n",
    "    log_pz = -zs**2/2 - 0.5*np.log(2*np.pi)\n",
    "    return (np.sum(log_p) + np.sum(minus_log_q) + np.sum(log_pz))/L\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_elbo(x, encoder, decoder, L=100, seed=0):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = elbo(x, encoder, decoder, L, seed)\n",
    "    return loss, tape.gradient(loss, [encoder.trainable_variables, decoder.trainable_variables])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=1000\n",
    "batch_size=32\n",
    "x = np.random.normal(0,1, size=(3,4,2))\n",
    "encoder, decoder = get_model(3,2,x.shape[1:],1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute '_in_graph_mode'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-cf287108698d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mxv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mloss_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_elbo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mi\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py\u001b[0m in \u001b[0;36mapply_gradients\u001b[0;34m(self, grads_and_vars, name)\u001b[0m\n\u001b[1;32m    433\u001b[0m         \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_hypers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_slots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/tensorflow/python/keras/optimizer_v2/adamax.py\u001b[0m in \u001b[0;36m_create_slots\u001b[0;34m(self, var_list)\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;31m# Separate for-loops to respect the ordering of slot variables from v1.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mvar\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvar_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_slot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'm'\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Create slots for the first moments.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mvar\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvar_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_slot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'v'\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Create slots for the second moments.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py\u001b[0m in \u001b[0;36madd_slot\u001b[0;34m(self, var, slot_name, initializer)\u001b[0m\n\u001b[1;32m    560\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mslot_name\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slot_names\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    561\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slot_names\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslot_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 562\u001b[0;31m     \u001b[0mvar_key\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_var_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    563\u001b[0m     \u001b[0mslot_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slots\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m     \u001b[0mweight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mslot_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslot_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py\u001b[0m in \u001b[0;36m_var_key\u001b[0;34m(var)\u001b[0m\n\u001b[1;32m    999\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_distributed_container\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1000\u001b[0m     \u001b[0mvar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_distributed_container\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1001\u001b[0;31m   \u001b[0;32mif\u001b[0m \u001b[0mvar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_graph_mode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1002\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mvar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shared_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1003\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mvar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_unique_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute '_in_graph_mode'"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    i = 0\n",
    "    while i < x.shape[0]:\n",
    "        xv = x[i:i+batch_size,:]\n",
    "        loss_value, grads = grad_elbo(xv, encoder, decoder)\n",
    "        optimizer.apply_gradients(zip(grads, [encoder.trainable_variables, decoder.trainable_variables]))\n",
    "        i += batch_size\n",
    "    if epoch % 100 == 0:\n",
    "        print(loss_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'dense/kernel:0' shape=(8, 3) dtype=float32, numpy=\n",
       " array([[-0.42738265,  0.3707028 ,  0.57056814],\n",
       "        [-0.01405627,  0.25074524,  0.65985304],\n",
       "        [-0.5002243 ,  0.09437931,  0.51558596],\n",
       "        [ 0.33064574,  0.19146824,  0.51699287],\n",
       "        [ 0.04061902, -0.10873175,  0.3906179 ],\n",
       "        [ 0.40254158,  0.3121826 ,  0.55093354],\n",
       "        [ 0.2201398 , -0.01859957, -0.3898437 ],\n",
       "        [ 0.6523438 , -0.6505063 ,  0.20486504]], dtype=float32)>,\n",
       " <tf.Variable 'dense/bias:0' shape=(3,) dtype=float32, numpy=array([0., 0., 0.], dtype=float32)>,\n",
       " <tf.Variable 'dense_1/kernel:0' shape=(3, 2) dtype=float32, numpy=\n",
       " array([[ 0.1548326 ,  0.52217317],\n",
       "        [-1.0591879 , -0.83329105],\n",
       "        [-0.47687507,  0.55437183]], dtype=float32)>,\n",
       " <tf.Variable 'dense_1/bias:0' shape=(2,) dtype=float32, numpy=array([0., 0.], dtype=float32)>,\n",
       " <tf.Variable 'dense_2_1/kernel:0' shape=(3, 2) dtype=float32, numpy=\n",
       " array([[-0.7061941 ,  0.6879364 ],\n",
       "        [-0.14369899,  0.5533023 ],\n",
       "        [-0.82900566,  0.11914051]], dtype=float32)>,\n",
       " <tf.Variable 'dense_2_1/bias:0' shape=(2,) dtype=float32, numpy=array([0., 0.], dtype=float32)>,\n",
       " <tf.Variable 'dense_3/kernel:0' shape=(2, 1) dtype=float32, numpy=\n",
       " array([[-0.7154134],\n",
       "        [-0.3947221]], dtype=float32)>,\n",
       " <tf.Variable 'dense_3/bias:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>,\n",
       " <tf.Variable 'dense_4/kernel:0' shape=(1, 8) dtype=float32, numpy=\n",
       " array([[-0.71738046,  0.47224128,  0.6511729 , -0.48157272, -0.11390352,\n",
       "         -0.1284349 ,  0.5199584 , -0.08199042]], dtype=float32)>,\n",
       " <tf.Variable 'dense_4/bias:0' shape=(8,) dtype=float32, numpy=array([0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>,\n",
       " <tf.Variable 'dense_5/kernel:0' shape=(1, 8) dtype=float32, numpy=\n",
       " array([[ 0.13380444,  0.75278807,  0.06623811, -0.202793  ,  0.6167605 ,\n",
       "         -0.18077141, -0.30039585,  0.51782167]], dtype=float32)>,\n",
       " <tf.Variable 'dense_5/bias:0' shape=(8,) dtype=float32, numpy=array([0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.trainable_variables + decoder.trainable_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-46.603218 -46.60321758116592 -46.60322114257292\n"
     ]
    }
   ],
   "source": [
    "def test_elbo_1():\n",
    "    x = np.random.normal(0,1, size=(3,4,2))\n",
    "    encoder, decoder = get_model(3,2,x.shape[1:],1)\n",
    "    L=5\n",
    "    seed=0\n",
    "    print (elbo(x, encoder, decoder, L,seed).numpy(),\n",
    "    manual_elbo(x, encoder, decoder, L, seed),\n",
    "    bad_elbo(x, encoder, decoder, L, seed))\n",
    "test_elbo_1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0903 09:08:11.294821 139768131282688 deprecation.py:323] From /home/dung/miniconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1205: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: id=734, shape=(), dtype=float32, numpy=-41.36326>, [[<tf.Tensor: id=948, shape=(8, 3), dtype=float32, numpy=\n",
      "array([[ 4.511608  , -0.19963868, -0.5984243 ],\n",
      "       [-2.3367553 ,  0.08292091,  0.00629209],\n",
      "       [ 5.466874  , -0.31411502, -0.19245052],\n",
      "       [ 0.26483363, -0.16757119,  0.9171245 ],\n",
      "       [ 3.5577078 , -0.26744398, -0.35635006],\n",
      "       [-2.018921  ,  0.21768755,  0.32522786],\n",
      "       [ 2.5862007 , -0.09949231, -1.0005776 ],\n",
      "       [ 4.8295403 , -0.18388346, -1.2133842 ]], dtype=float32)>, <tf.Tensor: id=949, shape=(3,), dtype=float32, numpy=array([-3.2936754 ,  0.22852534, -0.10238004], dtype=float32)>, <tf.Tensor: id=937, shape=(3, 2), dtype=float32, numpy=\n",
      "array([[-0.54802805,  3.387109  ],\n",
      "       [ 0.9924328 , -4.329119  ],\n",
      "       [-0.8883408 ,  3.639003  ]], dtype=float32)>, <tf.Tensor: id=934, shape=(2,), dtype=float32, numpy=array([-1.1085176,  4.637015 ], dtype=float32)>, <tf.Tensor: id=942, shape=(3, 2), dtype=float32, numpy=\n",
      "array([[-6.9688573,  2.022072 ],\n",
      "       [ 6.458977 , -2.1555984],\n",
      "       [-5.6916986,  1.8727942]], dtype=float32)>, <tf.Tensor: id=939, shape=(2,), dtype=float32, numpy=array([-6.5840025,  2.251316 ], dtype=float32)>], [<tf.Tensor: id=950, shape=(2, 1), dtype=float32, numpy=\n",
      "array([[-0.95649505],\n",
      "       [-7.336962  ]], dtype=float32)>, <tf.Tensor: id=951, shape=(1,), dtype=float32, numpy=array([3.7117195], dtype=float32)>, <tf.Tensor: id=902, shape=(1, 8), dtype=float32, numpy=\n",
      "array([[ 0.40073738,  0.2019277 ,  1.5337335 ,  1.3264362 ,  1.1542517 ,\n",
      "        -1.7574368 ,  0.15983383,  0.8196135 ]], dtype=float32)>, <tf.Tensor: id=899, shape=(8,), dtype=float32, numpy=\n",
      "array([-0.9061439 , -0.12513809, -4.157687  , -4.3613796 , -2.9857073 ,\n",
      "        4.147528  ,  0.40095484, -1.1069384 ], dtype=float32)>, <tf.Tensor: id=904, shape=(1, 8), dtype=float32, numpy=\n",
      "array([[ 0.5381947 ,  0.53297526, -1.0006056 , -0.896537  , -0.78786916,\n",
      "        -1.8809391 ,  0.6894525 , -0.3197302 ]], dtype=float32)>, <tf.Tensor: id=900, shape=(8,), dtype=float32, numpy=\n",
      "array([-1.8542207 , -1.8504134 ,  2.1644258 ,  3.8653018 ,  1.295496  ,\n",
      "        3.6514938 , -1.2725387 ,  0.95028406], dtype=float32)>]])\n"
     ]
    }
   ],
   "source": [
    "def test_grad_elbo_1():\n",
    "    x = np.random.normal(0,1, size=(3,4,2))\n",
    "    encoder, decoder = get_model(3,2,x.shape[1:],1)\n",
    "    L=5\n",
    "    seed=0\n",
    "    print (grad_elbo(x, encoder, decoder, L, seed))\n",
    "test_grad_elbo_1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_grad_elbo():\n",
    "    x = np.random.normal(0,1, size=(5,2))\n",
    "    encoder, decoder = get_model(2,1,x.shape[1:],2)\n",
    "    L=5\n",
    "    seed=0\n",
    "    eps = 0.000001\n",
    "    ERR_GRAD = 1\n",
    "    grad = grad_elbo(x, encoder, decoder, L, seed)[1]\n",
    "    elbo_at = elbo(x, encoder, decoder, L, seed)\n",
    "    encoder_weights = encoder.get_weights()\n",
    "    decoder_weights = decoder.get_weights()\n",
    "    weights = [encoder_weights, decoder_weights]\n",
    "    coders = (encoder, decoder)\n",
    "    encoder.set_weights(encoder_weights)\n",
    "    decoder.set_weights(decoder_weights)\n",
    "    for i in range(len(weights)):\n",
    "        for j in range(len(weights[i])):\n",
    "            layer = weights[i][j]\n",
    "            if len(layer.shape) == 1:\n",
    "                for k in range(len(layer)):\n",
    "                    layer[k] += eps\n",
    "                    elbo_near = elbo(x, encoder, decoder, L, seed)\n",
    "                    layer[k] -= eps\n",
    "                    numer_grad = (elbo_near - elbo_at)/eps\n",
    "                    assert(abs(grad[i][j].numpy()[k] - numer_grad) < ERR_GRAD)\n",
    "            else:\n",
    "                for k in range(len(layer)):\n",
    "                    for l in range(len(layer[k])):\n",
    "                        layer[k][l] += eps\n",
    "                        elbo_near = elbo(x, encoder, decoder, L, seed)\n",
    "                        layer[k][l] -= eps\n",
    "                        numer_grad = (elbo_near - elbo_at)/eps\n",
    "                        print(grad[i][j].numpy()[k][l], numer_grad)\n",
    "                        assert(abs(grad[i][j].numpy()[k][l] - numer_grad) < ERR_GRAD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adamax()\n",
    "global_step = tf.Variable(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(encoder.trainable_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
