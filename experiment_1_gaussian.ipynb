{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model: $X = Z + \\epsilon$ where $Z$ and $\\epsilon$ are independent unit normal. \\\n",
    "Then $p(x|z) \\approx \\mathcal{N}(z, 1)$ and $q(z|x) \\approx \\mathcal{N}(x/2,1/\\sqrt{2})$. \\\n",
    "Let's numerically verify this: \n",
    "1. Generate $M$ value pairs for $(X,Z)$ where $X=x$ and verify that $Z$ is approximately $\\mathcal{N}(x/2,1/\\sqrt{2})$\n",
    "2. Generate $M$ value pairs for $(X,Z)$ where $Z=z$ and verify that $X$ is approximately $\\mathcal{N}(z,1)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_fixed_z(value, num):\n",
    "    eps = np.random.normal(0,1, size=num)\n",
    "    return list(zip(eps +value, [value]*num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_fixed_x(value, num):\n",
    "    z_minus_eps = np.random.normal(0,1, size=num) - np.random.normal(0,1, size=num)\n",
    "    return list(zip([value]*num, z_minus_eps/2 + value/2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_p_x_z():\n",
    "    value = 2\n",
    "    num = 1000000\n",
    "    x_zs = gen_fixed_z(value, num)\n",
    "    xs = [x for (x,z) in x_zs]\n",
    "    assert(stats.jarque_bera(xs)[1] > .05)\n",
    "    plt.hist(xs, bins=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_p_x_z()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_q_z_x():\n",
    "    value = 2\n",
    "    num = 1000000\n",
    "    x_zs = gen_fixed_x(value, num)\n",
    "    zs = [z for (x,z) in x_zs]\n",
    "    assert(stats.jarque_bera(zs)[1] >.05)\n",
    "    plt.hist(zs, bins=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_q_z_x()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evidence lower bound analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the following recogition model: \\\n",
    "$ q(Z|X) \\sim \\mathcal{N}(a_1X + b_1, \\sigma_1) \\ $ and $\\ p(X|Z) \\sim \\mathcal{N}(a_2Z + b_2, \\sigma_2)$ \\\n",
    "and also $Z$ has unit normal prior. Then the evidence lower bound function is\n",
    "\n",
    "$$\\text{elbo}(x,z) = -\\frac{1}{2} \\, z^{2} + \\frac{{\\left(a_{1} x + b_{1} - z\\right)}^{2}}{2 \\, \\sigma_{1}^{2}} - \\frac{{\\left(a_{2} z + b_{2} - x\\right)}^{2}}{2 \\, \\sigma_{2}^{2}} + \\log\\left(\\frac{\\sqrt{2}}{2 \\, \\sqrt{\\pi}}\\right) - \\log\\left(\\frac{1}{\\sigma_{1}}\\right) + \\log\\left(\\frac{1}{\\sigma_{2}}\\right)$$\n",
    "\n",
    "First we expand:\n",
    "$$\\frac{a_{1}^{2} x^{2}}{2 \\, \\sigma_{1}^{2}} - \\frac{a_{2}^{2} z^{2}}{2 \\, \\sigma_{2}^{2}} - \\frac{1}{2} \\, z^{2} + \\frac{a_{1} b_{1} x}{\\sigma_{1}^{2}} - \\frac{a_{2} b_{2} z}{\\sigma_{2}^{2}} - \\frac{a_{1} x z}{\\sigma_{1}^{2}} + \\frac{a_{2} x z}{\\sigma_{2}^{2}} + \\frac{b_{1}^{2}}{2 \\, \\sigma_{1}^{2}} - \\frac{b_{2}^{2}}{2 \\, \\sigma_{2}^{2}} + \\frac{b_{2} x}{\\sigma_{2}^{2}} - \\frac{x^{2}}{2 \\, \\sigma_{2}^{2}} - \\frac{b_{1} z}{\\sigma_{1}^{2}} + \\frac{z^{2}}{2 \\, \\sigma_{1}^{2}} + \\\\ \\log\\left(\\frac{\\sqrt{2}}{2 \\, \\sqrt{\\pi}}\\right) - \\log\\left(\\frac{1}{\\sigma_{1}}\\right) + \\log\\left(\\frac{1}{\\sigma_{2}}\\right)$$\n",
    "\n",
    "and then integrating over $z \\sim \\mathcal{N}(a_1x + b_1, \\sigma_1)$, with $Ez = a_1x + b_1$ amd $Ez^2 = (a_1x+b_1)^2 + \\sigma_1^2$\n",
    "\n",
    "\n",
    "$$\\newcommand{\\Bold}[1]{\\mathbf{#1}}-\\frac{1}{2} \\, a_{1}^{2} x^{2} - \\frac{a_{1}^{2} a_{2}^{2} x^{2}}{2 \\, \\sigma_{2}^{2}} - a_{1} b_{1} x - \\frac{a_{1} a_{2}^{2} b_{1} x}{\\sigma_{2}^{2}} - \\frac{1}{2} \\, b_{1}^{2} - \\frac{1}{2} \\, \\sigma_{1}^{2} - \\frac{a_{2}^{2} b_{1}^{2}}{2 \\, \\sigma_{2}^{2}} - \\frac{a_{2}^{2} \\sigma_{1}^{2}}{2 \\, \\sigma_{2}^{2}} - \\frac{a_{1} a_{2} b_{2} x}{\\sigma_{2}^{2}} + \\frac{a_{1} a_{2} x^{2}}{\\sigma_{2}^{2}} - \\frac{a_{2} b_{1} b_{2}}{\\sigma_{2}^{2}} + \\frac{a_{2} b_{1} x}{\\sigma_{2}^{2}} - \\frac{b_{2}^{2}}{2 \\, \\sigma_{2}^{2}} + \\frac{b_{2} x}{\\sigma_{2}^{2}} - \\frac{x^{2}}{2 \\, \\sigma_{2}^{2}} + \\log\\left(\\sigma_{1}\\right) - \\log\\left(\\sigma_{2}\\right) + \\log\\left(\\frac{\\sqrt{2}}{2 \\, \\sqrt{\\pi}}\\right) + \\frac{1}{2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we integrate over $x \\sim \\mathcal{N}(0,2)$, with $Ex = 0\\ $ and $\\ Ex^2 = 2$ to get:\n",
    "$$F = \\newcommand{\\Bold}[1]{\\mathbf{#1}}-a_{1}^{2} - \\frac{1}{2} \\, b_{1}^{2} - \\frac{1}{2} \\, \\sigma_{1}^{2} - \\frac{a_{1}^{2} a_{2}^{2}}{\\sigma_{2}^{2}} - \\frac{a_{2}^{2} b_{1}^{2}}{2 \\, \\sigma_{2}^{2}} - \\frac{a_{2}^{2} \\sigma_{1}^{2}}{2 \\, \\sigma_{2}^{2}} - \\frac{a_{2} b_{1} b_{2}}{\\sigma_{2}^{2}} + \\frac{2 \\, a_{1} a_{2}}{\\sigma_{2}^{2}} - \\frac{b_{2}^{2}}{2 \\, \\sigma_{2}^{2}} - \\frac{1}{\\sigma_{2}^{2}} + \\log\\left(\\sigma_{1}\\right) - \\log\\left(\\sigma_{2}\\right) + \\log\\left(\\frac{\\sqrt{2}}{2 \\, \\sqrt{\\pi}}\\right) + \\frac{1}{2}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following are the partial derivatives:\n",
    "\\begin{eqnarray*}\n",
    "\\frac{\\partial{F}}{\\partial{a_1}} &=& -2 \\, a_{1} - \\frac{2 \\, a_{1} a_{2}^{2}}{\\sigma_{2}^{2}} + \\frac{2 \\, a_{2}}{\\sigma_{2}^{2}} \\\\\n",
    "\\frac{\\partial{F}}{\\partial{a_2}} &=& -\\frac{2 \\, a_{1}^{2} a_{2}}{\\sigma_{2}^{2}} - \\frac{a_{2} b_{1}^{2}}{\\sigma_{2}^{2}} - \\frac{a_{2} \\sigma_{1}^{2}}{\\sigma_{2}^{2}} - \\frac{b_{1} b_{2}}{\\sigma_{2}^{2}} + \\frac{2 \\, a_{1}}{\\sigma_{2}^{2}} \\\\\n",
    "\\frac{\\partial{F}}{\\partial{b_1}} &=& \\newcommand{\\Bold}[1]{\\mathbf{#1}}-\\frac{2 \\, a_{1}^{2} b_{1}}{\\sigma_{2}^{2}} - \\frac{a_{2}^{2} b_{1}}{\\sigma_{2}^{2}} - \\frac{b_{1} \\sigma_{1}^{2}}{\\sigma_{2}^{2}} - \\frac{a_{2} b_{2}}{\\sigma_{2}^{2}} + \\frac{2 \\, a_{1}}{\\sigma_{2}^{2}}\\\\\n",
    "\\frac{\\partial{F}}{\\partial{b_2}} &=& \\newcommand{\\Bold}[1]{\\mathbf{#1}}-\\frac{a_{2} b_{1}}{\\sigma_{2}^{2}} - \\frac{b_{2}}{\\sigma_{2}^{2}} \\\\\n",
    "\\frac{\\partial{F}}{\\partial{\\sigma_1}} &=& \\newcommand{\\Bold}[1]{\\mathbf{#1}}-\\sigma_{1} - \\frac{b_{1}^{2} \\sigma_{1}}{\\sigma_{2}^{2}} + \\frac{1}{\\sigma_{1}} \\\\\n",
    "\\frac{\\partial{F}}{\\partial{\\sigma_2}} &=& \\newcommand{\\Bold}[1]{\\mathbf{#1}}\\frac{2 \\, a_{1}^{2} b_{1}^{2}}{\\sigma_{2}^{3}} + \\frac{a_{2}^{2} b_{1}^{2}}{\\sigma_{2}^{3}} + \\frac{b_{1}^{2} \\sigma_{1}^{2}}{\\sigma_{2}^{3}} + \\frac{2 \\, a_{2} b_{1} b_{2}}{\\sigma_{2}^{3}} - \\frac{4 \\, a_{1} b_{1}}{\\sigma_{2}^{3}} + \\frac{b_{2}^{2}}{\\sigma_{2}^{3}} - \\frac{1}{\\sigma_{2}} + \\frac{2}{\\sigma_{2}^{3}}\n",
    "\\end{eqnarray*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that $a_1 = 1/2, a_2 = 1, b_1 = b_2 = 0, \\sigma_1 = 1/\\sqrt{2}, \\sigma_2 = 1$ is a stationary point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will run tensorflow to see if it can discover this point by maximizing $F$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(a1,a2,b1,b2,sigma1,sigma2):\n",
    "    return (-a1**2 - 1/2*b1**2 - 1/2*sigma1**2 - a1**2*a2**2/sigma2**2 \n",
    "            -1/2*a2**2*b1**2/sigma2**2 - 1/2*a2**2*sigma1**2/sigma2**2\n",
    "            -a2*b1*b2/sigma2**2 + 2*a1*a2/sigma2**2 - 1/2*b2**2/sigma2**2\n",
    "            -1/sigma2**2 + tf.math.log(sigma1) - tf.math.log(sigma2)\n",
    "            + tf.math.log(1/2*tf.sqrt(2.0)/tf.sqrt(np.pi)) + 1/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8, 0.7, 0.7, 0.8, 0.9, 1.4, -4.9189386]\n",
      "[0.26774672, 0.53549397, -2.9368648e-21, 6.7602745e-21, 0.9255394, 1.308911, -1.7655122]\n",
      "[0.26774672, 0.53549397, -3.6871e-41, 8.4872e-41, 0.9255394, 1.308911, -1.7655122]\n",
      "[0.26774672, 0.53549397, -8e-45, 1.5e-44, 0.9255394, 1.308911, -1.7655122]\n",
      "[0.26774672, 0.53549397, -8e-45, 1.5e-44, 0.9255394, 1.308911, -1.7655122]\n",
      "[0.26774672, 0.53549397, -8e-45, 1.5e-44, 0.9255394, 1.308911, -1.7655122]\n",
      "[0.26774672, 0.53549397, -8e-45, 1.5e-44, 0.9255394, 1.308911, -1.7655122]\n",
      "[0.26774672, 0.53549397, -8e-45, 1.5e-44, 0.9255394, 1.308911, -1.7655122]\n",
      "[0.26774672, 0.53549397, -8e-45, 1.5e-44, 0.9255394, 1.308911, -1.7655122]\n",
      "[0.26774672, 0.53549397, -8e-45, 1.5e-44, 0.9255394, 1.308911, -1.7655122]\n"
     ]
    }
   ],
   "source": [
    "a1 = tf.Variable(1.0)\n",
    "b1 = tf.Variable(1.0)\n",
    "a2 = tf.Variable(1.0)\n",
    "b2 = tf.Variable(1.0)\n",
    "sigma1 = tf.Variable(1.0)\n",
    "sigma2 = tf.Variable(1.0)\n",
    "epochs = 10000\n",
    "optimizer = tf.keras.optimizers.Adamax()\n",
    "learning_rate = 0.1\n",
    "for epoch in range(epochs):\n",
    "    with tf.GradientTape() as t:\n",
    "        t.watch([a1, a2, b1, b2, sigma1, sigma2])\n",
    "        F = loss(a1,a2,b1,b2,sigma1,sigma2)\n",
    "        da1, da2, db1, db2, dsigma1, dsigma2 = t.gradient(F, [a1, a2, b1, b2, sigma1, sigma2])\n",
    "        a1.assign_add(learning_rate*da1)\n",
    "        a2.assign_add(learning_rate*da2)\n",
    "        b1.assign_add(learning_rate*db1)\n",
    "        b2.assign_add(learning_rate*db2)\n",
    "        sigma1.assign_add(learning_rate*dsigma1)\n",
    "        sigma2.assign_add(learning_rate*dsigma2)\n",
    "        if epoch % 1000 == 0:\n",
    "            print([x.numpy() for x in [a1,a2,b1,b2,sigma1,sigma2,F]])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(-1.7655122, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.set_floatx('float32')\n",
    "print(loss(0.5,1.0,0.0,0.0,1/tf.sqrt(2.0),1.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many optimal solutions and the analytical solution above is one of them.\\\n",
    "However, the model actually imposes more constraints on the parameters than we have used.\\\n",
    "We can see that\n",
    "\\begin{eqnarray*}\n",
    "EX &=& E_Z(a_2Z+b_2) = b_2 \\\\\n",
    "EX^2 &=& E_Z(a_2Z^2+b_2) =  \\sigma_2^2 + a_2^2 + b_2^2 \\\\\n",
    "EZ^2 &=& E_X(EZ^2|X) = E_X(a_1X + b_1X)^2 + \\sigma_2^2 \\\\\n",
    "&=& a_1^2EX^2  + 2a_1b_1EX + b_1^2 + \\sigma_1^2 \\\\\n",
    "&=& a_1^2(\\sigma_2^2 + a_2^2 + b_2^2) + 2a_1b_1b_2 + b_1^2 + \\sigma_1^2 = 1\n",
    "\\end{eqnarray*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will replace $\\sigma_1^2$ by $1 - a_1^2(\\sigma_2^2 + a_2^2 + b_2^2) - 2a_1b_1b_2 - b_1^2$ and try the optimization again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sigma1(a1,a2,b1,b2,sigma2):\n",
    "    return tf.sqrt(1.0 - a1**2*(sigma2**2 + a2**2 + b2**2) - 2*a1*b1*b2 - b1**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss2(a1,a2,b1,b2,sigma2):\n",
    "    sigma1 = get_sigma1(a1,a2,b1,b2,sigma2)\n",
    "    return loss(a1,a2,b1,b2,sigma1,sigma2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def constraint(a1,a2,b1,b2,sigma1,sigma2):\n",
    "    return a1**2*(sigma2**2 + a2**2 + b2**2) + 2*a1*b1*b2 + b1**2 + sigma1**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.099657714, 0.10892073, 0.08876026, 0.088986024, 0.9891274, 1.0981951, -1.9251027]\n",
      "[0.07588988, 0.15177853, -5.249129e-05, 0.00036689814, 0.9942241, 1.4060409, -1.7655122]\n",
      "[0.07588961, 0.15177909, -3.4159007e-07, 2.3015466e-06, 0.9942241, 1.4060446, -1.7655122]\n",
      "[0.07588961, 0.15177909, -2.1447066e-09, 1.444878e-08, 0.9942241, 1.4060446, -1.7655122]\n",
      "[0.07588961, 0.15177909, -1.3464203e-11, 9.070763e-11, 0.9942241, 1.4060446, -1.7655122]\n",
      "[0.07588961, 0.15177909, -8.452661e-14, 5.6945133e-13, 0.9942241, 1.4060446, -1.7655122]\n",
      "[0.07588961, 0.15177909, -5.306476e-16, 3.574945e-15, 0.9942241, 1.4060446, -1.7655122]\n",
      "[0.07588961, 0.15177909, -3.3313396e-18, 2.244306e-17, 0.9942241, 1.4060446, -1.7655122]\n",
      "[0.07588961, 0.15177909, -2.091373e-20, 1.408947e-19, 0.9942241, 1.4060446, -1.7655122]\n",
      "[0.07588961, 0.15177909, -1.3129378e-22, 8.845191e-22, 0.9942241, 1.4060446, -1.7655122]\n"
     ]
    }
   ],
   "source": [
    "a1 = tf.Variable(0.1)\n",
    "b1 = tf.Variable(0.1)\n",
    "a2 = tf.Variable(0.1)\n",
    "b2 = tf.Variable(0.1)\n",
    "sigma2 = tf.Variable(1.0)\n",
    "epochs = 1000\n",
    "optimizer = tf.keras.optimizers.Adamax()\n",
    "learning_rate = 0.1\n",
    "for epoch in range(epochs):\n",
    "    with tf.GradientTape() as t:\n",
    "        t.watch([a1, a2, b1, b2, sigma2])\n",
    "        F = loss2(a1,a2,b1,b2,sigma2)\n",
    "        da1, da2, db1, db2, dsigma2 = t.gradient(F, [a1, a2, b1, b2, sigma2])\n",
    "        a1.assign_add(learning_rate*da1)\n",
    "        a2.assign_add(learning_rate*da2)\n",
    "        b1.assign_add(learning_rate*db1)\n",
    "        b2.assign_add(learning_rate*db2)\n",
    "        sigma2.assign_add(learning_rate*dsigma2)\n",
    "        if epoch % 100 == 0:\n",
    "            sigma1 = get_sigma1(a1,a2,b1,b2,sigma2)\n",
    "            print([x.numpy() for x in [a1,a2,b1,b2,sigma1, sigma2,F]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We still couldn't get the desired solution. More constraints are needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elbo(x,z, a1, a2, b1, b2, sigma1, sigma2):\n",
    "    return (-z**2/2 + (a1*x + b1 -z)**2/2/sigma1**2 - (a2*z + b2 -x)**2/2/sigma2**2\n",
    "            + np.log(1/(np.sqrt(2*np.pi))) + np.log(sigma1) - np.log(sigma2) )\n",
    "\n",
    "def elbo_x(x, a1, a2, b1, b2, sigma1, sigma2):\n",
    "    return (-1/2*a1**2*x**2 - a1**2*a2**2*x**2/2/sigma2**2 - a1*b1*x\n",
    "            -a1*a2**2*b1*x/sigma2**2 - b1**2/2 - sigma1**2/2 - a2**2*b1**2/2/sigma2**2\n",
    "            -a2**2*sigma1**2/2/sigma2**2 - a1*a2*b2*x/sigma2**2 + a1*a2*x**2/sigma2**2\n",
    "            -a2*b1*b2/sigma2**2 + a2*b1*x/sigma2**2 -b2**2/2/sigma2**2 + b2*x/sigma2**2\n",
    "            -x**2/2/sigma2**2 + np.log(sigma1) - np.log(sigma2) - np.log(np.sqrt(2*np.pi)) + 1/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-1.7656506, -1.7656506, -1.7655122)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a1, a2, b1, b2, sigma1, sigma2 = 0.5, 1, 0.0, 0.0, 1.0/tf.sqrt(2.0), 1.0\n",
    "N = 10000000\n",
    "zs = np.random.normal(0,1, N)\n",
    "xs = zs + np.random.normal(0,1, N)\n",
    "res_x = elbo_x(xs, a1, a2, b1, b2, sigma1, sigma2)\n",
    "res = elbo(xs, zs, a1, a2, b1, b2, sigma1, sigma2)\n",
    "np.mean(res), np.mean(res_x), loss(a1,a2,b1,b2,sigma1,sigma2).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import model_normal_simple as mns\n",
    "import model_vae_bayes as mvb\n",
    "import elbo_calculator as ec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method model_normal_simple_layer.call of <model_normal_simple.model_normal_simple_layer object at 0x7fc2e110e518>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method model_normal_simple_layer.call of <model_normal_simple.model_normal_simple_layer object at 0x7fc2e110e518>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method model_normal_simple_layer.call of <model_normal_simple.model_normal_simple_layer object at 0x7fc2e008d9b0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method model_normal_simple_layer.call of <model_normal_simple.model_normal_simple_layer object at 0x7fc2e008d9b0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Layer model_normal_simple_layer is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "epoch 200: loss = 2.5350924\n",
      "epoch 400: loss = 2.0355387\n",
      "epoch 600: loss = 1.8560082\n",
      "epoch 800: loss = 1.7926022\n",
      "epoch 1000: loss = 1.7723292\n",
      "epoch 1200: loss = 1.7654363\n",
      "epoch 1400: loss = 1.762422\n",
      "epoch 1600: loss = 1.7611587\n",
      "epoch 1800: loss = 1.7604368\n",
      "epoch 2000: loss = 1.7600085\n",
      "epoch 2200: loss = 1.7596731\n",
      "epoch 2400: loss = 1.7594357\n",
      "epoch 2600: loss = 1.7593004\n",
      "epoch 2800: loss = 1.7591975\n",
      "epoch 3000: loss = 1.7591598\n",
      "epoch 3200: loss = 1.7591368\n",
      "epoch 3400: loss = 1.7591159\n",
      "epoch 3600: loss = 1.7591282\n",
      "epoch 3800: loss = 1.759124\n",
      "epoch 4000: loss = 1.7591246\n",
      "epoch 4200: loss = 1.7591327\n",
      "epoch 4400: loss = 1.7591305\n",
      "epoch 4600: loss = 1.759123\n",
      "epoch 4800: loss = 1.7591277\n",
      "epoch 5000: loss = 1.7591296\n",
      "epoch 5200: loss = 1.7591289\n",
      "epoch 5400: loss = 1.7591329\n",
      "epoch 5600: loss = 1.7591325\n",
      "epoch 5800: loss = 1.7591321\n",
      "epoch 6000: loss = 1.7591306\n",
      "epoch 6200: loss = 1.759128\n",
      "epoch 6400: loss = 1.7591236\n",
      "epoch 6600: loss = 1.759133\n",
      "epoch 6800: loss = 1.7591518\n",
      "epoch 7000: loss = 1.7591254\n",
      "epoch 7200: loss = 1.7591379\n",
      "epoch 7400: loss = 1.7591271\n",
      "epoch 7600: loss = 1.7591318\n",
      "epoch 7800: loss = 1.7591285\n",
      "epoch 8000: loss = 1.7591189\n",
      "epoch 8200: loss = 1.7591156\n",
      "epoch 8400: loss = 1.7591317\n",
      "epoch 8600: loss = 1.7591214\n",
      "epoch 8800: loss = 1.7591462\n",
      "epoch 9000: loss = 1.7591254\n",
      "epoch 9200: loss = 1.7591296\n",
      "epoch 9400: loss = 1.7591367\n",
      "epoch 9600: loss = 1.759125\n",
      "epoch 9800: loss = 1.759133\n",
      "epoch 10000: loss = 1.7591412\n"
     ]
    }
   ],
   "source": [
    "epochs=10000\n",
    "batch_size=32\n",
    "zs = np.random.normal(0,1, (1000,1))\n",
    "xs = zs + np.random.normal(0,1, (1000,1))\n",
    "model = mns.model_normal_simple()\n",
    "elbo_cal = ec.elbo_calculator(model, xs)\n",
    "optimizer = tf.keras.optimizers.Adamax()\n",
    "options = {'length': 100, 'seed': 0}\n",
    "for epoch in range(epochs):\n",
    "    loss_elbo = lambda : elbo_cal.get_elbo(options)\n",
    "    optimizer.minimize(loss_elbo, model.get_trainable_variables())\n",
    "    if (epoch+1) % 200 == 0:\n",
    "        print('epoch %s: loss = %s' %(epoch + 1, loss_elbo().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([<tf.Variable 'model_normal_simple_layer/a:0' shape=() dtype=float32, numpy=0.616099>,\n",
       "  <tf.Variable 'model_normal_simple_layer/b:0' shape=() dtype=float32, numpy=0.013964243>,\n",
       "  <tf.Variable 'model_normal_simple_layer/sigma:0' shape=() dtype=float32, numpy=0.50141793>],\n",
       " [0.616099, 0.013964243, 0.50141793])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.encoder.trainable_variables, model.encoder.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'model_normal_simple_layer_1/a:0' shape=() dtype=float32, numpy=1.2162175>,\n",
       " <tf.Variable 'model_normal_simple_layer_1/b:0' shape=() dtype=float32, numpy=-0.021237144>,\n",
       " <tf.Variable 'model_normal_simple_layer_1/sigma:0' shape=() dtype=float32, numpy=0.704466>]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.decoder.trainable_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "a1,b1,sigma1,a2,b2,sigma2= *model.encoder.get_weights(), *model.decoder.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=7629406, shape=(), dtype=float32, numpy=-1.765667>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss(a1,a2,b1,b2,sigma1,sigma2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.7657305414100763"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = 10000000\n",
    "zs = np.random.normal(0,1, N)\n",
    "xs = zs + np.random.normal(0,1, N)\n",
    "res = elbo(xs, zs, a1, a2, b1, b2, sigma1, sigma2)\n",
    "np.mean(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
